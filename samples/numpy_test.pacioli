# ------------------------------------------------------------------------------
# Numpy/Tensorflow examples
#
# Experiment with different representations of samples
# ------------------------------------------------------------------------------

import si;
import numpy/iris;

# ------------------------------------------------------------------------------
# Nearest neigbor
# ------------------------------------------------------------------------------

declare nn_fit :: (List(Tuple(Feature!unit, Index(Target))))
                    -> Tuple(List(Feature!unit), List(Index(Target)));

declare nearest_neighbor :: (List(Feature!unit), Feature!unit) -> 1;

define norm(x) =
  sqrt(inner(x, x));

define distance(x, y) =
  norm((x-y) * weight);

define nearest_neighbor(data_list, x) =
  begin
    best := 0;
    best_dist := distance(x, nth(0, data_list));
    n := list_size(data_list);
    i := 1;
    while i != n do
      dist := distance(x, nth(i, data_list));
      if dist < best_dist then
          best_dist := dist;
          best := i;
      end
      i := i + 1;
    end
    return best;
  end;

define nn_fit(samples) =
  tuple([data | (data, target) <- samples],
        [target | (data, target) <- samples]);

define nn_predict(classifier, case) =
  let (data, target) = classifier in
    nth(nearest_neighbor(data, case), target)
  end;

# ------------------------------------------------------------------------------
# Split train/test
# ------------------------------------------------------------------------------

declare split_train_test ::
  for_type t: (List(t), 1) -> Tuple(List(t), List(t));

define split_train_test(data, prob) =
  begin
    train := [];
    test := [];
    n := list_size(data);
    i := 0;
    while i != n do
        elt := nth(i, data);
        if random() < prob then
            test := cons(elt, test);
        else
            train := cons(elt, train);
        end
        i := i + 1;
    end
    return tuple(train, test);
  end;

# ------------------------------------------------------------------------------
# Feature weights
# ------------------------------------------------------------------------------

defunit point "pt";

defmatrix weight :: point / Feature!unit = {
  "Sepal length" -> 1,
  "Sepal width" -> 1,
  "Petal length" -> 1,
  "Petal width" -> 1
};

# ------------------------------------------------------------------------------
# Test input
# ------------------------------------------------------------------------------

defmatrix my_test :: Feature!unit = {
  "Sepal length" -> 5.1,
  "Sepal width" -> 3.5,
  "Petal length" -> 1.4,
  "Petal width" -> 0.2
};

# ------------------------------------------------------------------------------
# Tests on numpy data (works only for target numpy)
# ------------------------------------------------------------------------------

define numpy_classifier =
  apply(knn_fit, numpy_iris_array);

#knn_predict(numpy_classifier, my_test);

# ------------------------------------------------------------------------------
# Tests on hardcode copy of numpy data (works for any target)
# ------------------------------------------------------------------------------

define list_classifier =
  nn_fit(iris_list);

nn_predict(list_classifier, my_test);

define run_experiment(samples) =
  let prob = 10/100,
      (train, test) = split_train_test(samples, prob),
      classifier = nn_fit(train),
      predictions = [ pred | (features, target) <- test,
                              predicted := nn_predict(classifier, features),
                              pred := tuple(predicted, target)],
      nr_trials = list_size(predictions),
      nr_correct = sum[1 | (x, y) <- predictions, x = y]
  in
    begin
      print("");
      print("");
      write("Experiment (test/train probability=");
      write(to_percentage(prob));
      print(")");
    
      print("trials");
      print(nr_trials);
      print("correct");
      print(nr_correct);
      print("accuracy");
      print(to_percentage(nr_correct/nr_trials));

      i := 0;
      n := list_size(predictions);
      print("");
      print("Predicted, Actual");
      while i != n do
        (predicted, actual) := nth(i, predictions);
        write(predicted);
        write(", ");
        print(actual);
        i := i + 1;
      end
    end
  end;

run_experiment(iris_list);

